<!DOCTYPE html>
<!-- This webpage is built based on http://web.cs.ucla.edu/~kwchang. Many thanks to Professor Kai-Wei Chang. -->
<html lang="en"><!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2016 --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Jinlan Fu</title>

  <meta name="author" content="This webpage is built based on http://web.cs.ucla.edu/~kwchang. Many thanks to Professor Kai-Wei Chang.">

  

  <link rel="alternate" type="application/rss+xml" title="Jinlan Fu" href="https://jinlanfu.github.io/">

  
    
      <link rel="stylesheet" href="./JinlanFu/font-awesome.min.css">
    
  

  
    
      <link rel="stylesheet" href="./JinlanFu/bootstrap.min.css">
    
      <link rel="stylesheet" href="./JinlanFu/bootstrap-social.css">
    
      <link rel="stylesheet" href="./JinlanFu/main.css">
    
  

  
    
      <link rel="stylesheet" href="./JinlanFu/css">
    
      <link rel="stylesheet" href="./JinlanFu/css(1)">
    
  

  

  

  

    <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="Jinlan Fu">
  

  

</head>


  <body>
  
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">      
          <li>
            <a href="https://jinlanfu.github.io">Home</a>
          </li>
          <li>
          <a href="Publications.htm">Publications</a>
          </li>
          <li>            
          <a href="Awards.htm">Awards</a>
          </li>
          <li>  
          <a href="News.htm">News</a>
          </li>
          <li>
          <a href="Collaborators.htm">Collaborators & Opening</a>
          </li>     
      </ul>
    </div>
  </div>
  

  </div>
</nav>

    <!-- TODO this file has become a mess, refactor it -->




<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
        <div class="center">
            <center>
        <div class="page-heading">
          <h2>Publications</h2>
		  
		    
            <!--<hr class="small"> -->
            <!-- <span class="page-subheading"><a href="http://web.cs.ucla.edu/~kwchang/publications_area/"><b>[See recent papers on selected topics]</b></a></span> -->
			
		  
            </div></center>
	

	  
		  
        </div>
      </div>
    </div>
  </div>

</header>





<div class="container" role="main">
  <div class="row">
    <div class="col-lg-10 col-lg-offset-1 col-md-10 col-md-offset-1">
<div style="text-align: left">
<a target="_blank" href="https://scholar.google.com/citations?user=rXB6z5QAAAAJ&hl=en">
<h3 id="academic">My Google Scholar 
<span class="ai ai-google-scholar ai-lg" style="color:#3c58ad" aria-hidden="true">
</span></h3></a> 
</div>
 <h4> * denotes the corresponding author. </h4>
<hr>






<h2 class="bibliography">2022</h2>
<ul class="bibliography">




<li>
<h4> <a href="https://arxiv.org/pdf/2204.14264.pdf"> Polyglot Prompt: Multilingual Multitask PrompTraining </a></h4>
<span id="fu2022poly"> <b>Jinlan Fu</b>, See-Kiong Ng, Pengfei Liu </span>
<br>
    <span class="conf">EMNLP </span>
    <a href="https://arxiv.org/pdf/2204.14264.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/jinlanfu/Polyglot_Prompt" class="my_code">Code</a> 
    <a data-toggle="collapse" href="#fu2022poly-abstract" class="my_details">Abstract</a>
<a data-toggle="collapse" href="#fu2022poly-bibtex" class="my_details">BibTeX</a>

<div id="fu2022poly-materials">
  
  <pre id="fu2022poly-abstract" class="pre collapse">This paper aims for a potential architectural improvement for multilingual learning and asks: Can different tasks from different languages be modeled in a monolithic framework, i.e. without any task/language-specific module? The benefit of achieving this could open new doors for future multilingual research, including allowing systems trained on low resources to be further assisted by other languages as well as other tasks. We approach this goal by developing a learning framework named Polyglot Prompting to exploit prompting methods for learning a unified semantic space for different languages and tasks with multilingual prompt engineering. We performed a comprehensive evaluation of 6 tasks, namely topic classification, sentiment classification, named entity recognition, question answering, natural language inference, and summarization, covering 24 datasets and 49 languages. The experimental results demonstrated the efficacy of multilingual multitask prompt-based learning and led to inspiring observations. We also present an interpretable multilingual evaluation methodology and show how the proposed framework, multilingual multitask prompt training, works. We release all datasets prompted in the best setting and code. </pre>
  <pre id="fu2022poly-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2022poly,
  title = {Polyglot Prompt: Multilingual Multitask PrompTraining},
  author = {Jinlan Fu, See-Kiong Ng, Pengfei Liu},
  booktitle = {EMNLP},
  year = {2022}
}
</pre> 
</div>
</li>






<li>
<h4> <a href="https://aclanthology.org/2022.coling-1.38.pdf"> CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations </a></h4>
<span id="lin2022corefdiffs"> Lin Xu, Qixian Zhou, <b>Jinlan Fu</b>, Min-Yen Kan, See-Kiong Ng </span>
<br>
    <span class="conf">COLING </span>
    <a href="https://aclanthology.org/2022.coling-1.38.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/cathyxl/coref-diffs" class="my_code">Code</a>   
    <a data-toggle="collapse" href="#lin2022corefdiffs-abstract" class="my_details">Abstract</a>
<a data-toggle="collapse" href="#lin2022corefdiffs-bibtex" class="my_details">BibTeX</a>

<div id="lin2022corefdiffs-materials">
  
  <pre id="lin2022corefdiffs-abstract" class="pre collapse">Knowledge-grounded dialog systems need to incorporate smooth transitions among knowledge selected for generating responses, to ensure that dialog flows naturally. For document-grounded dialog systems, the inter- and intra-document knowledge relations can be used to model such conversational flows. We develop a novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture the inter-document relationships based on commonsense and similarity and the intra-document co-referential structures of knowledge segments within the grounding documents. We propose CorefDiffs, a Co-referential and Differential flow management method, to linearize the static Coref-MDG into conversational sequence logic. CorefDiffs performs knowledge selection by accounting for contextual graph structures and the knowledge difference sequences. CorefDiffs significantly outperforms the state-of-the-art by 9.5%, 7.4%, and 8.2% on three public benchmarks. This demonstrates that the effective modeling of co-reference and knowledge difference for dialog flows are critical for transitions in document-grounded conversation </pre>
  <pre id="lin2022corefdiffs-bibtex" class="pre pre-scrollable collapse">@inproceedings{lin2022corefdiffs,
  title = {CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations},
  author = {Lin Xu, Qixian Zhou, Jinlan Fu, Min-Yen Kan, See-Kiong Ng},
  booktitle = {COLING},
  year = {2022}
}
</pre> 
</div>
</li>









<li>
<h4> <a href="https://arxiv.org/pdf/2205.02129.pdf"> Are All the Datasets in Benchmark Necessary? A Pilot Study of Dataset Evaluation for Text Classification </a></h4>
<span id="xiao2022eval"> Yang Xiao, <b>Jinlan Fu*</b>, See-Kiong Ng, Pengfei Liu </span>
<br>
    <span class="conf">NAACL </span>
    <a href="https://arxiv.org/pdf/2205.02129.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/ExpressAI/DataLab" class="my_code">Code</a>
    <a href="https://datalab.nlpedia.ai/" class="my_code">DataLab</a>      
    <a data-toggle="collapse" href="#xiao2022eval-abstract" class="my_details">Abstract</a>
<a data-toggle="collapse" href="#xiao2022eval-bibtex" class="my_details">BibTeX</a>

<div id="xiao2022eval-materials">
  
  <pre id="xiao2022eval-abstract" class="pre collapse">In this paper, we ask the research question of whether all the datasets in the benchmark are necessary. We approach this by first characterizing the distinguishability of datasets when comparing different systems. Experiments on 9 datasets and 36 systems show that several existing benchmark datasets contribute little to discriminating top-scoring systems, while those less used datasets exhibit impressive discriminative power. We further, taking the text classification task as a case study, investigate the possibility of predicting dataset discrimination based on its properties (e.g., average sentence length). Our preliminary experiments promisingly show that given a sufficient number of training experimental records, a meaningful predictor can be learned to estimate dataset discrimination over unseen datasets. We released all datasets with features explored in this work on DataLab </pre>
  <pre id="xiao2022eval-bibtex" class="pre pre-scrollable collapse">@inproceedings{xiao2022eval,
  title = {Are All the Datasets in Benchmark Necessary? A Pilot Study of Dataset Evaluation for Text Classification},
  author = {Yang Xiao, Jinlan Fu, See-Kiong Ng, Pengfei Liu},
  booktitle = {NAACL},
  year = {2022}
}
</pre> 
</div>
</li>






<li>
<h4> <a href="https://aclanthology.org/2022.acl-demo.18.pdf"> DataLab: A Platform for Data Analysis and Intervention </a></h4>
<span id="xiao2022datalab"> Yang Xiao, <b>Jinlan Fu</b>, Weizhe Yuan, Vijay Viswanathan, Zhoumianze Liu, Yixin Liu, Graham Neubig, Pengfei Liu</span>
<br>
    <span class="conf">ACL-2022, Outstanding Demo </span>
    <a href="https://aclanthology.org/2022.acl-demo.18.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/ExpressAI/DataLab" class="my_code">Code</a>
    <a href="https://datalab.nlpedia.ai/" class="my_code">DataLab</a>      
    <a data-toggle="collapse" href="#xiao2022datalab-abstract" class="my_details">Abstract</a>
<a data-toggle="collapse" href="#xiao2022datalab-bibtex" class="my_details">BibTeX</a>

<div id="xiao2022datalab-materials">
  
  <pre id="xiao2022datalab-abstract" class="pre collapse">Despite data’s crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data.In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized interface so that many data processing operations can be provided within a unified interface. Additionally, in view of the ongoing surge in the proliferation of datasets, DataLab has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem. So far, DataLab covers 1,300 datasets and 3,583 of its transformed version, where 313 datasets support different types of analysis (e.g., with respect to gender bias) with the help of 119M samples annotated by 318 feature functions. DataLab is under active development and will be supported going forward. We have released a web platform, web API, Python SDK, and PyPI published package, which hopefully, can meet the diverse needs of researchers. </pre>
  <pre id="xiao2022datalab-bibtex" class="pre pre-scrollable collapse">@inproceedings{xiao2022datalab,
  title = {DataLab: A Platform for Data Analysis and Intervention},
  author = {Yang Xiao, Jinlan Fu, Weizhe Yuan, Vijay Viswanathan, Zhoumianze Liu, Yixin Liu, Graham Neubig, Pengfei Liu},
  booktitle = {ACL},
  year = {2022}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/1906.01378.pdf"> Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing </a></h4>

<span id="liu2021pretrain"> Pengfei Liu, Weizhe Yuan, <b>Jinlan Fu</b>, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig </span>
<br>
    <span class="conf">ACM Computing Surveys </span>
    <a href="https://arxiv.org/pdf/2107.13586.pdf" class="my_details">Full Text</a>
    <a href="http://pretrain.nlpedia.ai/" class="my_code">Resource</a>
    <a data-toggle="collapse" href="#liu2021pretrain-abstract" class="my_details">Abstract</a>
<a data-toggle="collapse" href="#liu2021pretrain-bibtex" class="my_details">BibTeX</a>
<div id="liu2021pretrain-materials">
  
  <pre id="liu2021pretrain-abstract" class="pre collapse">This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist. </pre>
  <pre id="liu2021pretrain-bibtex" class="pre pre-scrollable collapse">@inproceedings{liu2021pretrain,
  title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author = {Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig},
  booktitle = {ACM Computing Surveys},
  year = {2022}
}
</pre> 
</div>
</li>





</ul>











<h2 class="bibliography">2021</h2>
<ul class="bibliography">




<li>
<h4> <a href="https://arxiv.org/pdf/2104.07412.pdf"> XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation </a></h4>

<span id="ruder2021xtremer">Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, <b>Jinlan Fu</b>, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson </span>
<br>

    <span class="conf">EMNLP</span>

    <a href="https://arxiv.org/pdf/2104.07412.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/google-research/xtreme" class="my_code">Code</a>
    <a href="https://sites.research.google/xtreme/" class="my_code">Leaderboard</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/xtreme/" class="my_code">ExplainaBoard</a>

      
    <a data-toggle="collapse" href="#ruder2021xtremer-abstract" class="my_details">Abstract</a>
    
<a data-toggle="collapse" href="#ruder2021xtremer-bibtex" class="my_details">BibTeX</a>

<div id="ruder2021xtremer-materials">
  
  <pre id="ruder2021xtremer-abstract" class="pre collapse">Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.</pre>
  
  <pre id="ruder2021xtremer-bibtex" class="pre pre-scrollable collapse">@inproceedings{ruder2021xtremer,
  title = {XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation},
  author = {Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson},
  booktitle = {EMNLP},
  year = {2021}
}
</pre> 
</div>
</li>



<li>
<h4> <a href="https://arxiv.org/pdf/2104.06387.pdf"> EXPLAINABOARD: An Explainable Leaderboard for NLP </a></h4>

<span id="meng2020integer">Pengfei Liu, <b>Jinlan Fu</b>, Yang Xiao, Weizhe Yuan, Shuaicheng Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, Zi-Yi Dou, Graham Neubig</span>
<br>
    <span class="conf">ACL, Best Demo </span>

    <a href="https://arxiv.org/pdf/2104.06387.pdf" class="my_details">Full Text</a>

    <a href="https://github.com/neulab/ExplainaBoard" class="my_code">Code</a>

    <a href="http://explainaboard.nlpedia.ai/" class="my_code">ExplainaBoard</a>
      
    <a data-toggle="collapse" href="#liu2021explain-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#liu2021explain-bibtex" class="my_details">BibTeX</a>

<!-- <a href="http://web.cs.ucla.edu/~kwchang/bibliography/liu2021explain/" class="my_details">Details</a> -->
 

<div id="liu2021explain-materials">
  
  <pre id="liu2021explain-abstract" class="pre collapse">With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g.~what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g.~where does system A outperform system B? What if we combine systems A, B, and C?) and (iii) examine prediction results closely (e.g.~what are common errors made by multiple systems, or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps updated and is recently upgraded by supporting (1) multilingual multi-task benchmark, (2) meta-evaluation, and (3) more complicated task: machine translation, which reviewers also suggested.} We not only released an online platform on the website \url{http://explainaboard.nlpedia.ai/} but also make our evaluation tool an API with MIT Licence at Github \url{https://github.com/neulab/explainaBoard} and PyPi \url{https://pypi.org/project/interpret-eval/} that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate "output-driven" research in the future. </pre>
  

  <pre id="liu2021explain-bibtex" class="pre pre-scrollable collapse">@inproceedings{liu2021explain,
  title = {EXPLAINABOARD: An Explainable Leaderboard for NLP},
  author = {Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaicheng Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, Zi-Yi Dou, Graham Neubig},
  booktitle = {ACL},
  year = {2021}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/2106.00641.pdf"> SpanNER: Named Entity Re-/Recognition as Span Prediction </a></h4>

<span id="fu2021spanner"><b>Jinlan Fu</b>, Xuanjing Huang, Pengfei Liu </span>
<br>
    <span class="conf">ACL</span>
    
    <a href="https://arxiv.org/pdf/2106.00641.pdf" class="my_details">Full Text</a>

    <a href="https://github.com/neulab/spanner" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>
      
    <a data-toggle="collapse" href="#fu2021spanner-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2021spanner-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2021spanner-materials">
  
  <pre id="fu2021spanner-abstract" class="pre collapse">Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model's architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems' outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all code and datasets available: \url{https://github.com/neulab/spanner}, as well as an online system demo: \url{http://spanner.sh}. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: \url{http://explainaboard.nlpedia.ai/leaderboard/task-ner/}.</pre>
  

  <pre id="fu2021spanner-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2021spanner,
  title = {SpanNER: Named Entity Re-/Recognition as Span Prediction},
  author = {Jinlan Fu, Xuanjing Huang, Pengfei Liu},
  booktitle = {ACL},
  year = {2021}
}
</pre> 
</div>
</li>



<li>
<h4> <a href="https://arxiv.org/pdf/2104.04434.pdf"> Larger-Context Tagging: When and Why Does It Work? </a></h4>

<span id="fu2021spanner"><b>Jinlan Fu</b>, Liangjing Feng, Qi Zhang, Xuanjing Huang, Pengfei Liu </span>
<br>
    <span class="conf">NAACL</span>
    
    <a href="https://arxiv.org/pdf/2104.04434.pdf" class="my_details">Full Text</a>

    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>
    <a data-toggle="collapse" href="#fu2021larger-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2021larger-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2021larger-materials">
  
  <pre id="fu2021larger-abstract" class="pre collapse">The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information.</pre>
  

  <pre id="fu2021larger-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2021larger,
  title = {Larger-Context Tagging: When and Why Does It Work?},
  author = {Jinlan Fu, Liangjing Feng, Qi Zhang, Xuanjing Huang, Pengfei Liu},
  booktitle = {NAACL},
  year = {2021}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/2102.05486.pdf"> Towards More Fine-grained and Reliable NLP Performance Prediction </a></h4>

<span id="ye2021towards">Zihuiwen Ye, Pengfei Liu, <b>Jinlan Fu</b>, Graham Neubig </span>
<br>
    <span class="conf">EACL</span>
    
    <a href="https://arxiv.org/pdf/2102.05486.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/neulab/Reliable-NLPPP" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>

    <a data-toggle="collapse" href="#ye2021towards-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#ye2021towards-bibtex" class="my_details">BibTeX</a>

    
<div id="ye2021towards-materials">
  
  <pre id="ye2021towards-abstract" class="pre collapse">Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future. We make our code publicly available: \url{https://github.com/neulab/Reliable-NLPPP} </pre>
  

  <pre id="ye2021towards-bibtex" class="pre pre-scrollable collapse">@inproceedings{ye2021towards,
  title = {Towards More Fine-grained and Reliable NLP Performance Prediction},
  author = {Zihuiwen Ye, Pengfei Liu, Jinlan Fu, Graham Neubig},
  booktitle = {EACL},
  year = {2021}
}
</pre> 
</div>
</li>



<li>
<h4> <a href="https://arxiv.org/pdf/2104.07412.pdf"> Textflint: Unified multilingual robustness evaluation toolkit for natural language processing </a></h4>

<span id="wang2021textflint">Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, <b>Jinlan Fu</b>, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, Xuan-Jing Huang </span>
<br>

    <span class="conf">ACL</span>

    <a href="https://aclanthology.org/2021.acl-demo.41.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/textflint" class="my_code">Code</a>
    <a href="https://www.textflint.io/textflint" class="my_code">Textflint</a>
      
    <a data-toggle="collapse" href="#wang2021textflint-abstract" class="my_details">Abstract</a>
    
<a data-toggle="collapse" href="#wang2021textflint-bibtex" class="my_details">BibTeX</a>

<div id="wang2021textflint-materials">
  
  <pre id="wang2021textflint-abstract" class="pre collapse">TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their models from various aspects or to customize their evaluations as desired with just a few lines of code. TextFlint also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the model in terms of its robustness. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art deep learning models, classic supervised methods, and real-world systems. The toolkit is already available at https://github.com/textflint with all the evaluation results demonstrated at textflint.io.</pre>
  
  <pre id="wang2021textflint-bibtex" class="pre pre-scrollable collapse">@inproceedings{wang2021textflint,
  title = {TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing},
  author = {Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, Xuan-Jing Huang},
  booktitle = {ACL},
  year = {2021}
}
</pre> 
</div>
</li>


</ul>
<!-- </div>   -->



<h2 class="bibliography">2020</h2>
<ul class="bibliography">

<li>
<h4> <a href="https://arxiv.org/pdf/2011.06854.pdf"> Interpretable Multi-dataset Evaluation for Named Entity Recognition </a></h4>

<span id="fu2020interpret"><b>Jinlan Fu</b>, Pengfei Liu, Graham Neubig </span>
<br>
    <span class="conf">EMNLP</span>
    
    <a href="https://arxiv.org/pdf/2011.06854.pdf" class="my_details">Full Text</a>

    <a href="https://github.com/neulab/InterpretEval" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/" class="my_code">Demo</a>
    
    <a data-toggle="collapse" href="#fu2020interpret-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020interpret-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020interpret-materials">
  
  <pre id="fu2020interpret-abstract" class="pre collapse">With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval. </pre>
  

  <pre id="fu2020interpret-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020interpret,
  title = {Interpretable Multi-dataset Evaluation for Named Entity Recognition},
  author = {Jinlan Fu, Pengfei Liu, Graham Neubig},
  booktitle = {EMNLP},
  year = {2020}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/2011.06858.pdf"> RethinkCWS: Is Chinese Word Segmentation a Solved Task? </a></h4>

<span id="fu2020rethinkcws"> <b>Jinlan Fu</b>, Pengfei Liu, Qi Zhang, Xuanjing Huang </span>
<br>
    <span class="conf">EMNLP</span>
    
    <a href="https://arxiv.org/pdf/2011.06858.pdf" class="my_details">Full Text</a>
  <a href="https://github.com/neulab/InterpretEval" class="my_code">Code</a>
    <a href="http://explainaboard.nlpedia.ai/leaderboard/task-cws/" class="my_code">Demo</a>
    
    
    <a data-toggle="collapse" href="#fu2020rethinkcws-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020rethinkcws-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020rethinkcws-materials">
  
  <pre id="fu2020rethinkcws-abstract" class="pre collapse">The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what's left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user's models: https://github.com/neulab/InterpretEval.  </pre>
  

  <pre id="fu2020rethinkcws-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020rethinkcws,
  title = {RethinkCWS: Is Chinese Word Segmentation a Solved Task?},
  author = {Jinlan Fu, Pengfei Liu, Qi Zhang, Xuanjing Huang},
  booktitle = {EMNLP},
  year = {2020}
}
</pre> 
</div>
</li>






<li>
<h4> <a href="https://arxiv.org/pdf/2001.03844.pdf"> Rethinking Generalization of Neural Models: A Named Entity Recognition Case Study </a></h4>

<span id="fu2020rethinking"> <b>Jinlan Fu</b>, Pengfei Liu, Qi Zhang, Xuanjing Huang </span>
<br>
    <span class="conf">AAAI</span>
    
    <a href="https://arxiv.org/pdf/2001.03844.pdf" class="my_details">Full Text</a>
    <a href="http://pfliu.com/InterpretNER/interpretNER.html" class="my_code">Data</a>

    
    <a data-toggle="collapse" href="#fu2020rethinking-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020rethinking-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020rethinking-materials">
  
  <pre id="fu2020rethinking-abstract" class="pre collapse">While neural network-based models have achieved impressive performance on a large body of NLP tasks, the generalization behavior of different models remains poorly understood: Does this excellent performance imply a perfect generalization model, or are there still some limitations? In this paper, we take the NER task as a testbed to analyze the generalization behavior of existing models from different perspectives and characterize the differences of their generalization abilities through the lens of our proposed measures, which guides us to better design models and training methods. Experiments with in-depth analyses diagnose the bottleneck of existing neural NER models in terms of breakdown performance analysis, annotation errors, dataset bias, and category relationships, which suggest directions for improvement. We have released the datasets:(ReCoNLL, PLONER) for the future research at our project page: http://pfliu.com/InterpretNER/. </pre>
  

  <pre id="fu2020rethinking-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020rethinking,
  title = {Rethinking Generalization of Neural Models: A Named Entity Recognition Case Study.},
  author = {Jinlan Fu, Pengfei Liu, Qi Zhang, Xuanjing Huang},
  booktitle = {AAAI},
  year = {2020}
}
</pre> 
</div>
</li>





<li>
<h4> <a href="https://dl.acm.org/doi/10.1145/3336191.3371817"> Recurrent Memory Reasoning Network for Expert Finding in Community Question Answering </a></h4>

<span id="fu2020recurrent"> <b>Jinlan Fu</b>, Yi Li, Qi Zhang, Qinzhuo Wu, Renfeng Ma, Xuanjing Huang, Yu-Gang Jiang </span>
<br>
    <span class="conf">WSDM</span>
    
    <a href="https://dl.acm.org/doi/10.1145/3336191.3371817" class="my_details">Full Text</a>

    
    <a data-toggle="collapse" href="#fu2020recurrent-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#fu2020recurrent-bibtex" class="my_details">BibTeX</a>

    
<div id="fu2020recurrent-materials">
  
  <pre id="fu2020recurrent-abstract" class="pre collapse">Expert finding is a task designed to enable recommendation of the right person who can provide high-quality answers to a requester's question. Most previous works try to involve a content-based recommendation, which only superficially comprehends the relevance between a requester's question and the expertise of candidate experts by exploring the content or topic similarity between the requester's question and the candidate experts' historical answers. However, if a candidate expert has never answered a question similar to the requester's question, then existing methods have difficulty making a correct recommendation. Therefore, exploring the implicit relevance between a requester's question and a candidate expert's historical records by perception and reasoning should be taken into consideration. In this study, we propose a novel \textslrecurrent memory reasoning network (RMRN) to perform this task. This method focuses on different parts of a question, and accordingly retrieves information from the histories of the candidate expert.Since only a small percentage of historical records are relevant to any requester's question, we introduce a Gumbel-Softmax-based mechanism to select relevant historical records from candidate experts' answering histories. To evaluate the proposed method, we constructed two large-scale datasets drawn from Stack Overflow and Yahoo! Answer. Experimental results on the constructed datasets demonstrate that the proposed method could achieve better performance than existing state-of-the-art methods. </pre>
  

  <pre id="fu2020recurrent-bibtex" class="pre pre-scrollable collapse">@inproceedings{fu2020rethinkcws,
  title = {Recurrent Memory Reasoning Network for Expert Finding in Community Question Answering},
  author = {Jinlan Fu, Yi Li, Qi Zhang, Qinzhuo Wu, Renfeng Ma, Xuanjing Huang, Yu-Gang Jiang},
  booktitle = {WSDM},
  year = {2020}
}
</pre> 
</div>
</li>
</ul>







<h2 class="bibliography">2019</h2>
<ul class="bibliography">

<li>
<h4> <a href="https://arxiv.org/pdf/1906.01378.pdf"> Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning </a></h4>

<span id="peng2019distantly">Minlong Peng, Xiaoyu Xing, Qi Zhang, <b>Jinlan Fu</b>, Xuanjing Huang </span>
<br>
    <span class="conf">ACL</span>
    
    <a href="https://arxiv.org/pdf/1906.01378.pdf" class="my_details">Full Text</a>
    <a href="https://github.com/v-mipeng/LexiconNER" class="my_code">Code</a>

    <a data-toggle="collapse" href="#peng2019distantly-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#peng2019distantly-bibtex" class="my_details">BibTeX</a>

    
<div id="peng2019distantly-materials">
  
  <pre id="peng2019distantly-abstract" class="pre collapse">In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at \url{https://github.com/v-mipeng/LexiconNER}. </pre>
  

  <pre id="peng2019distantly-bibtex" class="pre pre-scrollable collapse">@inproceedings{peng2019distantly,
  title = {Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning},
  author = {Minlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, Xuanjing Huang},
  booktitle = {ACL},
  year = {2019}
}
</pre> 
</div>
</li>




<li>
<h4> <a href="https://arxiv.org/pdf/1905.12277.pdf"> Learning Task-specific Representation for Novel Words in Sequence Labeling </a></h4>

<span id="peng2019learning">Minlong Peng, Qi Zhang, Xiaoyu Xing, Tao Gui, <b>Jinlan Fu</b>, Xuanjing Huang </span>
<br>
    <span class="conf">IJCAI</span>
    
    <a href="https://arxiv.org/pdf/1905.12277.pdf" class="my_details">Full Text</a>
    
    <a data-toggle="collapse" href="#peng2019learning-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#peng2019learning-bibtex" class="my_details">BibTeX</a>

    
<div id="peng2019learning-materials">
  
  <pre id="peng2019learning-abstract" class="pre collapse">Word representation is a key component in neural-network-based sequence labeling systems. However, representations of unseen or rare words trained on the end task are usually poor for appreciable performance. This is commonly referred to as the out-of-vocabulary (OOV) problem. In this work, we address the OOV problem in sequence labeling using only training data of the task. To this end, we propose a novel method to predict representations for OOV words from their surface-forms (e.g., character sequence) and contexts. The method is specifically designed to avoid the error propagation problem suffered by existing approaches in the same paradigm. To evaluate its effectiveness, we performed extensive empirical studies on four part-of-speech tagging (POS) tasks and four named entity recognition (NER) tasks. Experimental results show that the proposed method can achieve better or competitive performance on the OOV problem compared with existing state-of-the-art methods. </pre>
  

  <pre id="peng2019learning-bibtex" class="pre pre-scrollable collapse">@inproceedings{peng2019learning,
  title = {Learning Task-specific Representation for Novel Words in Sequence Labeling},
  author = {Minlong Peng, Qi Zhang, Xiaoyu Xing, Tao Gui, Jinlan Fu, Xuanjing Huang},
  booktitle = {IJCAI},
  year = {2019}
}
</pre> 
</div>
</li>
</ul>






<h2 class="bibliography">2018</h2>
<ul class="bibliography">

<li>
<h4> <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16432/16127"> Adaptive Co-Attention Network for Named Entity Recognition in Tweets </a></h4>

<span id="zhang2018adaptive">Qi Zhang, <b>Jinlan Fu</b>, Xiaoyu Liu, Xuanjing Huang </span>
<br>
    <span class="conf">AAAI</span>
    
    <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16432/16127" class="my_details">Full Text</a>
    <a href="https://github.com/jlfu/NERmultimodal" class="my_code">Code</a>


    
    <a data-toggle="collapse" href="#zhang2018adaptive-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#zhang2018adaptive-bibtex" class="my_details">BibTeX</a>

    
<div id="zhang2018adaptive-materials">
  
  <pre id="zhang2018adaptive-abstract" class="pre collapse">In this study, we investigate the problem of named entity recognition for tweets. Named entity recognition is an important task in natural language processing and has been carefully studied in recent decades. Previous named entity recognition methods usually only used the textual content when processing tweets. However, many tweets contain not only textual content, but also images. Such visual information is also valuable in the name entity recognition task. To make full use of textual and visual information, this paper proposes a novel method to process tweets that contain multimodal information. We extend a bi-directional long short term memory network with conditional random fields and an adaptive co-attention network to achieve this task. To evaluate the proposed methods, we constructed a large scale labeled dataset that contained multimodal tweets. Experimental results demonstrated that the proposed method could achieve a better performance than the previous methods in most cases. </pre>
  

  <pre id="zhang2018adaptive-bibtex" class="pre pre-scrollable collapse">@inproceedings{zhang2018adaptive,
  title = {Adaptive Co-Attention Network for Named Entity Recognition in Tweets},
  author = {Qi Zhang, Jinlan Fu, Xiaoyu Liu, Xuanjing Huang},
  booktitle = {AAAI},
  year = {2018}
}
</pre> 
</div>
</li>



<li>
<h4> <a href="file:///Users/jinlanfu/Downloads/11959-Article%20Text-15487-1-2-20201228.pdf"> Neural Networks Incorporating Dictionaries for Chinese Word Segmentation </a></h4>

<span id="zhang2018neural">Qi Zhang, Xiaoyu Liu, <b>Jinlan Fu</b> </span>
<br>
    <span class="conf">AAAI</span>
    
    <a href="file:///Users/jinlanfu/Downloads/11959-Article%20Text-15487-1-2-20201228.pdf" class="my_details">Full Text</a>


    
    <a data-toggle="collapse" href="#zhang2018neural-abstract" class="my_details">Abstract</a>
    

<a data-toggle="collapse" href="#zhang2018neural-bibtex" class="my_details">BibTeX</a>

    
<div id="zhang2018neural-materials">
  
  <pre id="zhang2018neural-abstract" class="pre collapse">In recent years, deep neural networks have achieved significant success in Chinese word segmentation and many other natural language processing tasks. Most of these algorithms are end-to-end trainable systems and can effectively process and learn from large scale labeled datasets. However, these methods typically lack the capability of processing rare words and data whose domains are different from training data. Previous statistical methods have demonstrated that human knowledge can provide valuable information for handling rare cases and domain shifting problems. In this paper, we seek to address the problem of incorporating dictionaries into neural networks for the Chinese word segmentation task. Two different methods that extend the bi-directional long short-term memory neural network are proposed to perform the task. To evaluate the performance of the proposed methods, state-of-the-art supervised models based methods and domain adaptation approaches are compared with our methods on nine datasets from different domains. The experimental results demonstrate that the proposed methods can achieve better performance than other state-of-the-art neural network methods and domain adaptation approaches in most cases. </pre>
  

  <pre id="zhang2018neural-bibtex" class="pre pre-scrollable collapse">@inproceedings{zhang2018neural,
  title = {Neural Networks Incorporating Dictionaries for Chinese Word Segmentation},
  author = {Qi Zhang, Xiaoyu Liu, Jinlan Fu},
  booktitle = {AAAI},
  year = {2018}
}
</pre> 
</div>
</li>
</ul>






</div>  















































	    
    </div>
  </div>
</div>














<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=x4b_UiP0aJul00VZARygnAMRhiJr1F-uMHjJJAZb5-Y"></script>




    <footer>
  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="https://github.com/jinlanfu" title="GitHub">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
      
          <li>
            <a href="https://twitter.com/JinlanFu" title="Twitter">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
      
      
          <li>
            <a href="mailto:jinlanjonna@google.com" title="Email me">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
      
          <li>
            <a href="https://www.linkedin.com/in/jinlan-fu-401a6324a/?originalSubdomain=sg" title="LinkedIn">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
      
      
      
      
        </ul>
        <p class="copyright text-muted">
      Last Updated, Feb 2025
      </p>

          <!-- Please don't remove this, keep my open source work credited :) -->
    <p class="theme-by text-muted" style="color:#3c58ad;">
      Theme by
      <a href="http://deanattali.com/beautiful-jekyll/" style="color:#3c58ad;">beautiful-jekyll</a>
    </p>
      </div>
    </div>
  </div>
</footer>

  
    






  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script type="text/javascript" async="" src="./JinlanFu/js"></script><script async="" src="./JinlanFu/analytics.js"></script><script>
        if (typeof jQuery == 'undefined') {
          document.write('<script src="JinlanFu/jquery-1.11.2.min.js"></scr' + 'ipt>');
        }
      </script><script src="./JinlanFu/jquery-1.11.2.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
  <script src="./JinlanFu/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
  <script src="./JinlanFu/main.js"></script>
    
  



  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-74487606-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End Google Analytics -->


  
  

</body></html>